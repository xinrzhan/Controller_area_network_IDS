# -*- coding: utf-8 -*-
"""ID3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YxT4MC61Y7qzNCu_wcVrtlplUGCIUaFQ
"""

from google.colab import files
uploaded = files.upload()

#import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.datasets import load_iris
import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn import datasets
import io

df = pd.read_csv(io.BytesIO(uploaded['final_demo_n.csv']))
df['DataField'].fillna(0, inplace=True)

X = df.drop(columns=["Attack"])
print(X)

#determine there is no missing value in the attribute sample
X.isnull().values.any()

y = df["Attack"]
print(y)

y.isnull().values.any()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=100)
print("Spliting (training): ", X_train.shape)
print("Spliting (testing): ", X_test.shape)
print(X_train)

#compute entropy
def compute_entropy(y):
  if len(y) < 2:
    return 0
  freq = np.array(y.value_counts(normalize=True))
  return -(freq * np.log2(freq + 1e-6)).sum()

#calculate information gain
def compute_info_gain(samples, attr, target):
    values = samples[attr].value_counts(normalize=True)
    split_ent = 0
    for v, fr in values.iteritems():
        index = samples[attr]==v
        sub_ent = compute_entropy(target[index])
        split_ent += fr * sub_ent
    
    ent = compute_entropy(target)
    return ent - split_ent

#construct decision tree node
class TreeNode:
    def __init__(self, node_name="", min_sample_num=10, default_decision=None):
        self.children = {} # Sub nodes --
        # recursive, those elements of the same type (TreeNode)
        self.decision = None # Undecided
        self.split_feat_name = None # Splitting feature
        self.name = node_name
        self.default_decision = default_decision
        self.min_sample_num = min_sample_num

    def pretty_print(self, prefix=''):
        if self.split_feat_name is not None:
            for k, v in self.children.items():
                v.pretty_print(f"{prefix}:When {self.split_feat_name} is {k}")
                #v.pretty_print(f"{prefix}:{k}:")
        else:
            print(f"{prefix}:{self.decision}")

    def predict(self, sample):
        if self.decision is not None:
            # uncomment to get log information of code execution
            print("Decision:", self.decision)
            return self.decision
        else: 
            attr_val = sample[self.split_feat_name]
            # print(self.children)
            child = self.children[attr_val]
            # uncomment to get log information of code execution
            print("Testing ", self.split_feat_name, "->", attr_val)
            return child.predict(sample)

    def fit(self, X, y):
        if self.default_decision is None:
            self.default_decision = y.mode()[0]
        print(self.name, "received", len(X), "samples")
        if len(X) < self.min_sample_num:
            # If the data is empty when this node is arrived, 
            # we just make an arbitrary decision
            if len(X) == 0:
                self.decision = self.default_decision
                print("DECISION", self.decision)
            else:
                self.decision = y.mode()[0]
                print("DECISION", self.decision)
            return
        else: 
            unique_values = y.unique()
            if len(unique_values) == 1:
                self.decision = unique_values[0]
                print("DECISION", self.decision)
                return
            else:
                info_gain_max = 0
                for a in X.keys(): # Examine each attribute
                    aig = compute_info_gain(X, a, y)
                    if aig > info_gain_max:
                        info_gain_max = aig
                        self.split_feat_name = a
                print(f"Split by {self.split_feat_name}, IG: {info_gain_max:.2f}")
                self.children = {}
                for v in X[self.split_feat_name].unique():
                    index = X[self.split_feat_name] == v
                    self.children[v] = TreeNode(
                        node_name=self.name + ":" + self.split_feat_name + "==" + str(v),
                        min_sample_num=self.min_sample_num,
                        default_decision=self.default_decision)
                    self.children[v].fit(X[index], y[index])

#build a new class and call another class TreeNode() to accept training data in the fit() function
class ID3_Decision_Tree:
  def __init__(self):
    self.root = TreeNode()
dt = ID3_Decision_Tree()
dt.root.fit(X_train, y_train)

dt.root.pretty_print("")

#Test tree building
attrs = ['Length', 'ID', 'DataField']
data = df[attrs]
target = df["Attack"]
t = TreeNode() 
t.fit(data,target)

#Test tree working
for i in [0, 63, 116]:
    print(f"Test predict sample[{i}]: \n{data.iloc[i]}\n\tTarget: {target.iloc[i]}")
    print(f"Making decision ...")
    pred = t.predict(data.iloc[i])

#Fit the model in the decision tree classifier
classifier = DecisionTreeClassifier(criterion = 'entropy')
classifier.fit(X_train, y_train)

#Compute accuracy of prediction and build confusion matrix
y_prediction = classifier.predict(X_test)
matrix = confusion_matrix(y_test, y_prediction)
print("Accuracy:",metrics.accuracy_score(y_test, y_prediction))
print("Confusion Matrix: \n",  matrix)

#plot confusion matrix
plt.figure(figsize=(6,7))
sns.heatmap(data=matrix, linewidths=.3, linecolor='black',annot=True, square=True, cmap='Reds')
plt.ylabel('Actual')
plt.xlabel('Predict')
title = 'Accuracy Score: {}'.format(classifier.score(X_test, y_test))
plt.title(title, size = 12)

#visualize decision tree
from sklearn.tree import plot_tree
plt.figure(figsize = (20,20))
decision_tree_graph = plot_tree(decision_tree=classifier, max_depth=120, feature_names = df.columns, 
                     class_names =["Length", "ID", "DataField"] ,label = 'all', filled = True , precision = 3, rounded = True, impurity=True, fontsize=10)